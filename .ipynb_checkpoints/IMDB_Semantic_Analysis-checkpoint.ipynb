{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set base directory for dataset\n",
    "base_dir = 'imdb_dataset/'\n",
    "\n",
    "# Training dataset directories\n",
    "train_path = os.path.join(base_dir,'train')\n",
    "train_neg_path = os.path.join(train_path,'neg')\n",
    "train_pos_path = os.path.join(train_path,'pos')\n",
    "\n",
    "# Testing dataset directories\n",
    "test_path = os.path.join(base_dir,'test')\n",
    "test_neg_path = os.path.join(test_path,'neg')\n",
    "test_pos_path = os.path.join(test_path, 'pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Get size of negative & positive training data\\n# Negative reviews\\npath, dirs, files = next(os.walk(train_neg_path))\\nneg_file_count = len(files)\\n# Positive reviews\\npath, dirs, files = next(os.walk(train_pos_path))\\npos_file_count = len(files)\\n\\nneg_file_count, pos_file_count'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# Get size of negative & positive training data\n",
    "# Negative reviews\n",
    "path, dirs, files = next(os.walk(train_neg_path))\n",
    "neg_file_count = len(files)\n",
    "# Positive reviews\n",
    "path, dirs, files = next(os.walk(train_pos_path))\n",
    "pos_file_count = len(files)\n",
    "\n",
    "neg_file_count, pos_file_count\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORT DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "imdb_data_preprocessing : Method to club negative and positive movie reviews and store them in the required format for further processing\n",
    "    _dir : directory of the dataset to be processed\n",
    "\"\"\"\n",
    "\n",
    "def imdb_data_preprocessing(_dir, new_filename):\n",
    "    indices = []\n",
    "    text = []\n",
    "    rating = [] # 0 - negative, 1 - positive\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    # Positive movie reviews\n",
    "    for filename in os.listdir(_dir+'/pos'):\n",
    "        data = open(_dir+'/pos/'+filename, 'r', encoding=\"ISO-8859-1\").read()\n",
    "        indices.append(i)  \n",
    "        text.append(data)\n",
    "        rating.append(1)\n",
    "        i = i+1\n",
    "        \n",
    "    # Negative movie reviews\n",
    "    for filename in os.listdir(_dir+'/neg'):\n",
    "        data = open(_dir+'/neg/'+filename, 'r', encoding=\"ISO-8859-1\").read()\n",
    "        indices.append(i)\n",
    "        text.append(data)\n",
    "        rating.append(0)\n",
    "        i = i+1\n",
    "        \n",
    "    # Creating a dataset and storing it in the provided directory\n",
    "    Dataset = list(zip(indices, text,rating))\n",
    "    np.random.shuffle(Dataset)     # Shuffling dataset for the the better accuracy\n",
    "    df = pd.DataFrame(data=Dataset, columns = ['id', 'text', 'remark'])\n",
    "    df.to_csv(_dir+'/'+new_filename, index=False, header = True)\n",
    "    \n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training dataset by combining negative and positive reviews\n",
    "train_dataset_filename = 'train_dataset.csv'\n",
    "imdb_data_preprocessing(_dir=train_path, new_filename =train_dataset_filename )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>remark</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17076</td>\n",
       "      <td>What an insult to Olivia D'Abo who plays the f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1507</td>\n",
       "      <td>\"Citizen X\" is the superbly told true story of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6105</td>\n",
       "      <td>If the themes of The Girl From Missouri sound ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20805</td>\n",
       "      <td>I doubt this will ever even be a cult film. I ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8628</td>\n",
       "      <td>hello. i just watched this movie earlier today...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                               text  remark\n",
       "0  17076  What an insult to Olivia D'Abo who plays the f...       0\n",
       "1   1507  \"Citizen X\" is the superbly told true story of...       1\n",
       "2   6105  If the themes of The Girl From Missouri sound ...       1\n",
       "3  20805  I doubt this will ever even be a cult film. I ...       0\n",
       "4   8628  hello. i just watched this movie earlier today...       1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import training dataset\n",
    "training_data = pd.read_csv(os.path.join(train_path,train_dataset_filename))\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the shape of the sample size available for training\n",
    "training_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create testing dataset by combining negative and positive reviews\n",
    "test_dataset_filename = 'test_dataset.csv'\n",
    "imdb_data_preprocessing(_dir=test_path,new_filename=test_dataset_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>remark</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10332</td>\n",
       "      <td>Just after having moved into his new cottage i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4345</td>\n",
       "      <td>Mankind's Self awakening is the theme of \"2001...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19717</td>\n",
       "      <td>Christopher Lambert attracted me to this movie...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8473</td>\n",
       "      <td>One of the best romantic classic,teen deviyaan...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1372</td>\n",
       "      <td>Don't mind what this socially retarded person ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                               text  remark\n",
       "0  10332  Just after having moved into his new cottage i...       1\n",
       "1   4345  Mankind's Self awakening is the theme of \"2001...       1\n",
       "2  19717  Christopher Lambert attracted me to this movie...       0\n",
       "3   8473  One of the best romantic classic,teen deviyaan...       1\n",
       "4   1372  Don't mind what this socially retarded person ...       1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import testing dataset\n",
    "testing_data = pd.read_csv(os.path.join(test_path, test_dataset_filename))\n",
    "testing_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the shape of the testing data sample size\n",
    "testing_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets divide the traing and testing datasets into features and target (review) points\n",
    "X_train, y_train = training_data['text'].values, training_data['remark'].values\n",
    "X_test, y_test = testing_data['text'].values, testing_data['remark'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"What an insult to Olivia D'Abo who plays the film's heroine, Robin, to have Keanu Reeves appear so large on the box art of the film (and at least on recent reissues, to have only Reeves appear on the box), considering that she was the star. I realize that it is his name that will ultimately sell this long-forgotten After School Special, but at least give the woman some credit. <br /><br />Despite that, this has to be one of the worst teen sports-themed films that I have ever seen, and it strives very hard to add not only every teen and sports movie clichÃ© from the class warfare between the feuding gymnasts to the teen romance. And, in striving to somehow deliver itself as an amateur alternative of Flashdance (with the music in one of the warehouse dance scenes is even quite close to Michael Sembello's notable 'Maniac' which was made famous by Flashdance, or was it the other way around?). It includes similar dance sequences and worse yet, even the 80s dance and sports traditions of corny dance-offs between the heroine and her antagonist(s), the one who doubts her successes and abilities on the team. We saw this in Trashin' (a vert ramp joust) and Rad (BMX dancing at the prom, although it wasn't much for competition, but rather for fun) for example. In fact, this movie is chock full of unrealistic corniness, such as the somewhat homo-erotic rolling in the clothes at the Salvation Army with Robin and her friend from the team.<br /><br />Nonetheless, the film is about a young girl who comes from a rather poor background. To top it off in a massive need to squeeze from audiences as much sympathy as possible, she lives with her ailing mother, her obnoxious sister, and her careless (and slightly abusive) stepfather. Needless to say, homelife is not so appealing. Add to the mix, a talent for gymnastics, but several obstacles to joining the team (including the nuisance of her arrogant, snobby teammates, and a coach who also eventually doubts her abilities to compete well). And, of course, we can't forget that she's got eyes for one of the pretty boy preppies who is dating one of the obnoxious teammates, nor that she doesn't have a steady boyfriend (although Keanu as Tommy later enters the picture). Could this kid be any more pathetic? And it seems that one mess after another comes along to embarrass herself in her painfully long, redundant, and clichÃ©d quest to prove her worth to everyone.<br /><br />But, even the major moments of cheesiness which comprise most of the film, are hardly worth mentioning considering that the biggest distraction to this film is the horrible acting and dialog. (I like how the gym coach suddenly appears at the diner in the middle of the dance-off to scold the teammates). It makes episodes of 'Amazing Stories' look like Shakespeare.<br /><br />I imagine anyone able to locate this film and watch it these days is probably drawn to it mostly because of the nostalgic factor. For that you might be satisfied, but it is also an incredibly forced drama. So, Caveat Emptour.\"],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEXT PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\abhi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REGEXs to remove unwanted patterns from the text\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(text):\n",
    "    \n",
    "    # convert text string to lower case string\n",
    "    text = text.lower()\n",
    "    # replace symbols with a space in text\n",
    "    text = re.sub(REPLACE_BY_SPACE_RE,\" \", text)\n",
    "    # replace unwanted symbols from text\n",
    "    text = re.sub(BAD_SYMBOLS_RE,'', text)\n",
    "    # delete stopwords from text\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Training data text\n",
    "X_train = [text_preprocessing(x) for x in X_train]\n",
    "\n",
    "# Process Testing data text\n",
    "X_test = [text_preprocessing(x) for x in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['insult olivia dabo plays films heroine robin keanu reeves appear large box art film least recent reissues reeves appear box considering star realize name ultimately sell longforgotten school special least give woman credit br br despite one worst teen sportsthemed films ever seen strives hard add every teen sports movie clich class warfare feuding gymnasts teen romance striving somehow deliver amateur alternative flashdance music one warehouse dance scenes even quite close michael sembellos notable maniac made famous flashdance way around includes similar dance sequences worse yet even 80s dance sports traditions corny danceoffs heroine antagonist one doubts successes abilities team saw trashin vert ramp joust rad bmx dancing prom although wasnt much competition rather fun example fact movie chock full unrealistic corniness somewhat homoerotic rolling clothes salvation army robin friend teambr br nonetheless film young girl comes rather poor background top massive need squeeze audiences much sympathy possible lives ailing mother obnoxious sister careless slightly abusive stepfather needless say homelife appealing add mix talent gymnastics several obstacles joining team including nuisance arrogant snobby teammates coach also eventually doubts abilities compete well course cant forget shes got eyes one pretty boy preppies dating one obnoxious teammates doesnt steady boyfriend although keanu tommy later enters picture could kid pathetic seems one mess another comes along embarrass painfully long redundant clichd quest prove worth everyonebr br even major moments cheesiness comprise film hardly worth mentioning considering biggest distraction film horrible acting dialog like gym coach suddenly appears diner middle danceoff scold teammates makes episodes amazing stories look like shakespearebr br imagine anyone able locate film watch days probably drawn mostly nostalgic factor might satisfied also incredibly forced drama caveat emptour']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate frequency of words in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_freq_counts(text):\n",
    "    words_freq = {}\n",
    "    for line in text:\n",
    "        for word in line.split():\n",
    "            if word not in words_freq:\n",
    "                words_freq[word] = 1\n",
    "            else:\n",
    "                words_freq[word] = words_freq[word] + 1\n",
    "    return words_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data words (term) frequency\n",
    "words_counts = words_freq_counts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113007"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# size of unique words in the training dataset\n",
    "len(words_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('br', 59067),\n",
       " ('movie', 41941),\n",
       " ('film', 37559),\n",
       " ('one', 25561),\n",
       " ('like', 19690),\n",
       " ('good', 14615),\n",
       " ('even', 12537),\n",
       " ('would', 12141),\n",
       " ('time', 11840),\n",
       " ('really', 11673)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# most common terms in the copora\n",
    "most_common_words = sorted(words_counts.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "most_common_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BAG of WORDS Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "DICT_SIZE = 200000\n",
    "WORDS_TO_INDEX = {b[0]:a for a,b in enumerate(sorted(words_counts.items(),key = lambda x: x[1], reverse= True))}\n",
    "INDEX_TO_WORDS = {b:a for a,b in WORDS_TO_INDEX.items()}\n",
    "ALL_WORDS = WORDS_TO_INDEX.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "BAG_OF_WORDS creates a vector of tokens of the string\n",
    "    Inputs:\n",
    "        >> text: text to be vectorized\n",
    "        >> words_to_index: list of indices of words\n",
    "        dict_size: length of the words count\n",
    "\"\"\"\n",
    "def bag_of_words(text, words_to_index, dict_size):\n",
    "    # create a zero vector equaling the size of words list\n",
    "    result_vector = np.zeros(dict_size)\n",
    "    for word in set(text.split()):\n",
    "        if word in words_to_index:\n",
    "            result_vector[words_to_index[word]] = 1\n",
    "            \n",
    "    return result_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the BOW approach to train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse as sp_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape  (25000, 200000)\n",
      "X_test shape  (25000, 200000)\n"
     ]
    }
   ],
   "source": [
    "X_train_bow = sp_sparse.vstack([sp_sparse.csr_matrix(bag_of_words(text, WORDS_TO_INDEX, DICT_SIZE)) for text in X_train])\n",
    "X_test_bow = sp_sparse.vstack([sp_sparse.csr_matrix(bag_of_words(text, WORDS_TO_INDEX, DICT_SIZE)) for text in X_test])\n",
    "\n",
    "print('X_train shape ', X_train_bow.shape)\n",
    "print('X_test shape ', X_test_bow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets calculate number of non-zero elements in a given row\n",
    "\n",
    "row_ = X_train_bow[9].toarray()[0]\n",
    "\n",
    "non_zero_elements = np.nonzero(row_)\n",
    "\n",
    "non_zero_elements_count = len(non_zero_elements[0])\n",
    "\n",
    "non_zero_elements_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf_features(X_train,X_test):\n",
    "    \n",
    "    tf_idf_vectorizer = TfidfVectorizer(min_df=5, max_df=0.9,ngram_range=(1,2), token_pattern='(\\S+)')\n",
    "    features = tf_idf_vectorizer.fit(X_train)\n",
    "    \n",
    "    X_train = features.transform(X_train)\n",
    "    X_test = features.transform(X_test)\n",
    "    \n",
    "    return X_train, X_test, tf_idf_vectorizer.vocabulary_\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf, X_test_tfidf, tfidf_vocab = tf_idf_features(X_train,  X_test)\n",
    "tfidf_reversed_vocab = {i:word for word,i in tfidf_vocab.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will be using Stochaistic stochastic gradient descent classifier\n",
    "# We don't need to process out target data (y_train and y_test) as it is already in binary form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BOW apporach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhi\\Anaconda3\\envs\\ml_study_env\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "classifier_bow = SGDClassifier(loss=\"hinge\", penalty=\"l1\", n_iter=20)\n",
    "classifier_bow.fit(X_train_bow, y_train)\n",
    "y_pred_bow = classifier_bow.predict(X_test_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF_IDF approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhi\\Anaconda3\\envs\\ml_study_env\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "classifier_tfidf = SGDClassifier(loss=\"hinge\", penalty=\"l1\", n_iter=20)\n",
    "classifier_tfidf.fit(X_train_tfidf, y_train)\n",
    "y_pred_tfidf = classifier_tfidf.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_scores(y_, predicted):\n",
    "    print('Accuracy Score: {}%'.format((accuracy_score(y_, predicted, normalize = False)*100)/len(y_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag-of-words\n",
      "Accuracy Score: 84.928%\n"
     ]
    }
   ],
   "source": [
    "print('Bag-of-words')\n",
    "accuracy_scores(y_test, y_pred_bow.round())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF\n",
      "Accuracy Score: 87.58%\n"
     ]
    }
   ],
   "source": [
    "print('TF-IDF')\n",
    "accuracy_scores(y_test, y_pred_tfidf.round())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
